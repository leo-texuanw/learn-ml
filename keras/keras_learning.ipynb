{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "in model: what is there structure like?\n",
    "when do we use L1 regularization and when l2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple model\n",
    "## Sequential model\n",
    "A model is (usually) a graph of layers. The most common type of model is a stack of layers: the `tf.keras.Sequential` model.\n",
    "\n",
    "To build a simple, fully-connected network (i.e. multi-layer perceptron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Adds a densely-connected layer with 64 units to the model\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add another\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ReLU and Softmax Activation Functions](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions)\n",
    "#### Sigmoid\n",
    "Lose knowledge from data. Deeper your network is, more knowledge from the data will be \"lost\".\n",
    "\n",
    "- So usually not for hidden layer\n",
    "\n",
    "#### Rectified Linear Units\n",
    "Simplest non-linear activation function.\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "    \n",
    "- ReLUs result in much faster training for large networks.\n",
    "- Unfortunately, ReLU units can be fragile during training and can \"die\". Weights will be zero forever after a point.\n",
    "- With a proper setting of the learning rate this is less frequently an issue.\n",
    "\n",
    "#### Softmax function\n",
    "Squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1.\n",
    "\n",
    "$$\\sigma(z)_j = \\frac{e^{z_j}}{\\sum^K_{k=1}e^{z_k}}$$\n",
    "`z` is a vector of the inputs to the output layer (if you have 10 output units, then there are 10 elements in `z`). And `j` indexes the output units, so $j = 1, 2, ..., K$.\n",
    "\n",
    "While sigmoid only suitable for 2 classes, softmax can be used for hundereds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the layers\n",
    "There are many `tf.keras.layers` available with some common constructor parameters:\n",
    "\n",
    "- *activation*: Set the activation function for the layer. This parameter is specified by the name of a built-in function or as a callable object. By default, no activation is applied.\n",
    "\n",
    "- *kernel_initializer* and *bias_initializer*: The initialization schemes that create the layer's weights (kernel and bias). This parameter is a name or a callable object. This defaults to the \"Glorot uniform\" initializer.\n",
    "\n",
    "- *kernel_regularizer* and *bias_regularizer*: The regularization schemes that apply the layer's weights (kernel and bias), such as L1 or L2 regularization. By default, no regularization is applied.\n",
    "\n",
    "The following instantiates `tf.keras.layers.Dense` layers using constructor arguments:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a sigmoid layer:\n",
    "keras.layers.Dense(64, activation='sigmoid')\n",
    "# Or\n",
    "keras.layers.Dense(64, activation=tf.sigmoid)\n",
    "\n",
    "# A linear layer with L1 regularization of factor 0.01 applied to the\n",
    "# `kernel matrix`:\n",
    "keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the\n",
    "# `bias vector`:\n",
    "keras.layers.Dense(64, bias_regularizer=keras.regularizers.l2(0.01)\n",
    "\n",
    "# A linear layer with a kernel initialized to\n",
    "# a random orthogonal matrix:\n",
    "keras.layers.Dense(64, kernel_initializer='orthogonal')\n",
    "# A linear layer with a bias vector initialized to 2.0s\n",
    "keras.layers.Dense(64, bias_initializer=keras.initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "A regression model that uses L1 regularization technique is called **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.\n",
    "$$\\sum^n_{i=1}(y_i - \\hat y_i)^2 +\\lambda\\sum^P_{j=1}|\\beta_j|$$\n",
    "\n",
    "That with L2 is called **Ridge Regression** which adds “squared magnitude” of coefficient as penalty term to the loss function.\n",
    "$$\\sum^n_{i=1}(y_i - \\hat y_i)^2 +\\lambda\\sum^P_{j=1}\\beta_j^2$$\n",
    "\n",
    "If *lambda* is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit.\n",
    "\n",
    "Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, \n",
    "- a huge number of features: L1 or L2 regularization\n",
    "- a small set of features: cross-validation, stepwise regression, etc..\n",
    "\n",
    "Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate\n",
    "## Set up training\n",
    "After the model is constructed, configure its learning process by calling the compile method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.Model.compile` takes three important arguments:\n",
    "\n",
    "- `optimizer`: This object specifies the training procedure. Pass it optimizer instances from the `tf.train` module, such as `AdamOptimizer`, `RMSPropOptimizer`, or `GradientDescentOptimizer`.\n",
    "- `loss`: The function to minimize during optimization. Common choices include `mean square error (mse)`, `categorical_crossentropy`, and `binary_crossentropy`. Loss functions are specified by name or by passing a callable object from the `tf.keras.losses module`.\n",
    "- `metrics`: Used to monitor training. These are string names or callables from the `tf.keras.metrics` module.\n",
    "\n",
    "The following shows a few examples of configuring a model for training:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Configure a model for mean-squared error regression.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])  # mean absolute error\n",
    "\n",
    "# Configure a model for categorical classification\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Numpy data\n",
    "For small datasets, use in-memory *Numpy* arrays to train and evaluate a model. The model is \"fit\" to the training data using the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "#model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "#model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "#model.predict(data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.Model.fit` takes three important arguments:\n",
    "- `epochs`: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n",
    "- `batch_size`: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training. This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n",
    "- `validation_data`: When prototyping a model, you want to easily monitor its performance on some validation data. Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 263us/step - loss: 11.5773 - acc: 0.0960 - val_loss: 11.4955 - val_acc: 0.1600\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 11.5493 - acc: 0.1270 - val_loss: 11.4877 - val_acc: 0.0800\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5433 - acc: 0.1290 - val_loss: 11.4858 - val_acc: 0.1300\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 11.5383 - acc: 0.1550 - val_loss: 11.4895 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 11.5330 - acc: 0.1390 - val_loss: 11.4882 - val_acc: 0.0900\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5295 - acc: 0.1690 - val_loss: 11.4893 - val_acc: 0.1000\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 11.5240 - acc: 0.1530 - val_loss: 11.4866 - val_acc: 0.0900\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 11.5216 - acc: 0.1620 - val_loss: 11.4867 - val_acc: 0.1000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5168 - acc: 0.1620 - val_loss: 11.5005 - val_acc: 0.1000\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 11.5132 - acc: 0.1660 - val_loss: 11.4948 - val_acc: 0.1000\n",
      "1000/1000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10935095, 0.10236258, 0.09818337, ..., 0.09453773, 0.10262468,\n",
       "        0.09647124],\n",
       "       [0.131173  , 0.09998714, 0.09044728, ..., 0.1107971 , 0.08764174,\n",
       "        0.11112654],\n",
       "       [0.10746676, 0.1036151 , 0.09376768, ..., 0.1043117 , 0.11596734,\n",
       "        0.09098008],\n",
       "       ...,\n",
       "       [0.09077893, 0.08651523, 0.10472265, ..., 0.12508228, 0.11357807,\n",
       "        0.10846335],\n",
       "       [0.08848815, 0.10286779, 0.0975915 , ..., 0.11754835, 0.0990286 ,\n",
       "        0.10251258],\n",
       "       [0.08824801, 0.10511488, 0.09578815, ..., 0.09402472, 0.09972115,\n",
       "        0.09904332]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))\n",
    "\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "model.predict(data, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input tf.data datasets\n",
    "Use the *Datasets API* to scale to large datasets or multi-device training. Pass a `tf.data.Dataset` instance to the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 11.4885 - acc: 0.1646\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4670 - acc: 0.1813\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4882 - acc: 0.1896\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4978 - acc: 0.1885\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4875 - acc: 0.2031\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4850 - acc: 0.1927\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4383 - acc: 0.2042\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4907 - acc: 0.2115\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4670 - acc: 0.2156\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4797 - acc: 0.2115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11bd30810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates a toy dataset instance:\n",
    "#print(data.shape)\n",
    "#data = tf.constant(1,shape=[1000,32])\n",
    "#print (data.shape)\n",
    "#labels = tf.constant(1,shape=[1000,1])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "# Don't forget to specify `steps_per_epoch` when calling `fit`\n",
    "# on a dataset.\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30)\n",
    "\n",
    "# model.evaluate(dataset, steps=30)\n",
    "\n",
    "# model.predict(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 11.5320 - acc: 0.1167 - val_loss: 11.4144 - val_acc: 0.1354\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5113 - acc: 0.1146 - val_loss: 11.9047 - val_acc: 0.1146\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 999us/step - loss: 11.5378 - acc: 0.1187 - val_loss: 12.0930 - val_acc: 0.1875\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5497 - acc: 0.1135 - val_loss: 12.0609 - val_acc: 0.1458\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5438 - acc: 0.1167 - val_loss: 11.4111 - val_acc: 0.1146\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5450 - acc: 0.1156 - val_loss: 11.8978 - val_acc: 0.1771\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5028 - acc: 0.1146 - val_loss: 12.0935 - val_acc: 0.1562\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5579 - acc: 0.1167 - val_loss: 12.0608 - val_acc: 0.1250\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5380 - acc: 0.1146 - val_loss: 11.4136 - val_acc: 0.0729\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.5547 - acc: 0.1250 - val_loss: 11.9021 - val_acc: 0.1458\n",
      "30/30 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5212436 , 0.5286048 , 0.5968594 , ..., 0.5587488 , 0.5999805 ,\n",
       "        0.59404695],\n",
       "       [0.5781163 , 0.58532274, 0.46443263, ..., 0.5537153 , 0.5532794 ,\n",
       "        0.50749433],\n",
       "       [0.5642902 , 0.62500095, 0.491225  , ..., 0.57367074, 0.57789665,\n",
       "        0.53011835],\n",
       "       ...,\n",
       "       [0.54619735, 0.57631195, 0.506721  , ..., 0.5471295 , 0.6129462 ,\n",
       "        0.52478355],\n",
       "       [0.5426198 , 0.5788304 , 0.52312434, ..., 0.5604776 , 0.568208  ,\n",
       "        0.5508774 ],\n",
       "       [0.6686078 , 0.599493  , 0.48047528, ..., 0.62443024, 0.49602535,\n",
       "        0.5199309 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30,\n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=3)\n",
    "\n",
    "model.evaluate(dataset, steps=30)\n",
    "\n",
    "model.predict(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and predict\n",
    "The `tf.keras.Model.evaluate` and `tf.keras.Model.predict` method can use Numpy data and a `tf.data.Dataset`.\n",
    "\n",
    "To evaluate the inference-mode loss and metrics for the data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build advanced models\n",
    "## Functional API\n",
    "The `tf.keras.Sequenial` model is a simple stack of layers that cannot represent arbitrary models.\n",
    "Use the [Keras functional API](https://keras.io/getting-started/functional-api-guide/) to build complex model topologies such as:\n",
    "- Multi-input models\n",
    "- Multi-output models\n",
    "- Models with shared layers (the same layer called several times)\n",
    "- Models with non-sequential data flows (e.g. residual connections)\n",
    "\n",
    "Using functional API to build a simple, fully-connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 256us/step - loss: 11.7041 - acc: 0.1040\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.6016 - acc: 0.1050\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.5705 - acc: 0.1000\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 11.5596 - acc: 0.1260\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 11.5523 - acc: 0.1190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11c223450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,)) # Returns a placeholder tensor\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Instantiate the model given inputs and outputs.\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# The compile step specifies the training configuration\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing\n",
    "Build a fully-customizable model by subclassing `tf.keras.Model` and defining your own forward pass. Create layers in the `__init__` method and set them as attributes of the class instance. Define the forward pass in the call method.\n",
    "\n",
    "Model subclassing is particularly useful when *eager execution* is enabled since the forward pass can be written imperatively.\n",
    "\n",
    "> With more flexibility, more opportunities for user errors comes. If possible, prefer the functional API\n",
    "\n",
    "A subclassed `tf.keras.Model` using a custom forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define your laysers here.\n",
    "        self.dense_1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        ''' Define your forward pass here,\n",
    "            Using layers you previously defined (in `__init__`).\n",
    "        '''\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # you need to override this function if you want to use the\n",
    "        # subclassed model as part of a functional-style model.\n",
    "        # Ohterwise, this method is optional.\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 227us/step - loss: 11.6372 - acc: 0.1100\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 11.6094 - acc: 0.1130\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 11.5747 - acc: 0.1010\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 11.5602 - acc: 0.0990\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5549 - acc: 0.0940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11bd53950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiates the subclassed model\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers\n",
    "Create a custom layer by subclassing `tf.keras.layers.Layer` and implementing the following methods:\n",
    "- `build`: Create the weights of the layer. Add weights with the `add_weight` method.\n",
    "- `call`: Define the forward pass.\n",
    "- `compute_output_shape`: Specify how to compute the output shape of the layer given the input shape.\n",
    "- Optionally, a layer can be serializerd by implementing the `get_config` method and the `from_config` class method.\n",
    "\n",
    "An example of a custom layer that implements a `matmul` of an input with a kernel matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        # Be sure to call this at the end\n",
    "        super(MyLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.output_dim\n",
    "        return tf.TensorShape(shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 236us/step - loss: 13.9603 - acc: 0.0940\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 13.0030 - acc: 0.0930\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 12.3383 - acc: 0.1020\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 12.0652 - acc: 0.1120\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 11.9821 - acc: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11b976810>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model using the custom layer\n",
    "model1 = keras.Sequential([MyLayer(10),\n",
    "                           keras.layers.Activation('softmax')])\n",
    "\n",
    "# The compile step specifies the training configuration\n",
    "model1.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model1.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "A callback is an object passed to a model to customize and extend its behavior during training. You can write your own custom callback, or use the built-in `tf.keras.callbacks` that include:\n",
    "\n",
    "- `tf.keras.callbacks.ModelCheckpoint`: Save checkpoints of your model at regular intervals.\n",
    "- `tf.keras.callbacks.LearningRateSchedular`: Dynamically change the learning rate.\n",
    "- `tf.keras.callbacks.EarlyStopping`: Interrupt traning when validation performance has stopped improving.\n",
    "- `tf.keras.callbakcs.TensorBoard`: Monitor the model's behavior using Tensorboard.\n",
    "\n",
    "To use a `tf.keras.callbacks.Callback`, pass it to the model's `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.8306 - acc: 0.1140 - val_loss: 11.8247 - val_acc: 0.0600\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 11.8109 - acc: 0.1210 - val_loss: 11.8020 - val_acc: 0.0700\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.7904 - acc: 0.1120 - val_loss: 11.7859 - val_acc: 0.0700\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 11.7728 - acc: 0.1150 - val_loss: 11.7676 - val_acc: 0.0600\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 11.7559 - acc: 0.1200 - val_loss: 11.7459 - val_acc: 0.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10c247050>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "    keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "model1.fit(data, labels, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "         validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and restore\n",
    "## Weights only\n",
    "Save and load the weights of a model in the *TensorFlow checkpoint* file format using `tf.keras.Model.save_weights`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x11d00be10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights to a TensorFlow Checkpoint file\n",
    "model1.save_weights('./my_model_weights')\n",
    "\n",
    "# Restore the model's state,\n",
    "# this requires a model with the same *architecture*.\n",
    "model1.load_weights('my_model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights can also be saved to the *Keras HDF5* format (the default for the multi-backend implementation of Keras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "`save_weights` requires h5py when saving in hdf5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-023f8aaf422c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save weights to a HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./my_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Restore the model's state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leo/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       raise ImportError(\n\u001b[0;32m-> 1347\u001b[0;31m           '`save_weights` requires h5py when saving in hdf5.')\n\u001b[0m\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       \u001b[0mcheck_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_weights` requires h5py when saving in hdf5."
     ]
    }
   ],
   "source": [
    "# Save weights to a HDF5 file\n",
    "model1.save_weights('./my_model_weights.h5', save_format='h5')\n",
    "\n",
    "# Restore the model's state\n",
    "model1.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Only\n",
    "A model's configuration can be saved - this serializes the model architecture without any weights. A saved configuration can recreate and initialize the same model, even without the code that defined the original model.\n",
    "\n",
    "> *Caution*: Subclassed models are not serializable because their architecture is defined by the Python code in the body of the *call* method.\n",
    "\n",
    "Keras supports JSON and YAML serialization formats:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-6961442fc53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Serialize a model to JSON format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjson_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Recreate the model (freshly initialized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfresh_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leo/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1497\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leo/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/keras/engine/network.pyc\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m     model_config = {\n\u001b[1;32m   1468\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leo/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/keras/engine/sequential.pyc\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m       config.append({\n\u001b[1;32m    284\u001b[0m           \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m           \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m       })\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-5adc85f40ce2>\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mbase_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMyLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mbase_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_dim'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# Serialize a model to JSON format\n",
    "json_string = model.to_json()\n",
    "\n",
    "# Recreate the model (freshly initialized)\n",
    "fresh_model = keras.models.from_json(json_string)\n",
    "\n",
    "# Serialize a model to YAML format\n",
    "yaml_string = model.to_yaml()\n",
    "\n",
    "# Recreate the model\n",
    "fresh_model = keras.models.from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire model\n",
    "The entire model can be saved to a file that contains the weight values, the model' sconfiguration, and even the optimizer's configuration.\n",
    "\n",
    "This allows you to checkpoint a model and resume training later - from the exact same state - without access to the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-keyword arg after keyword arg (<ipython-input-35-f814aecb0eb6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-f814aecb0eb6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    keras.layers.Dense(10, activation='softmax', input_shape(32,32)),\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-keyword arg after keyword arg\n"
     ]
    }
   ],
   "source": [
    "# Create a trival model\n",
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(10, activation='softmax', input_shape(32,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='rmsprop',\n",
    "               loss='categrorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "model2.fit(data, labels, batch_size=32, epochs=5)\n",
    "\n",
    "# Save entire model to a HDF5 file\n",
    "model2.save('my_model.h5')\n",
    "\n",
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model2 = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager execution\n",
    "**Eager execution** is an imperative programming environment that evaluates operations immediately. This is not required for keras, but is supported by `tf.keras` and useful for inspecting your program and debugging.\n",
    "\n",
    "All of the tf.keras model-building APIs are compatible with eager execution. And while the Sequential and functional APIs can be used, eager execution especially benefits model subclassing and building custom layers—the APIs that require you to write the forward pass as code (instead of the APIs that create models by assembling existing layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution\n",
    "## Estimators\n",
    "The Estimators API is used for training models for distributed environments. This targets industry use cases such as distributed training on large datasets that can export a model for production.\n",
    "\n",
    "A `tf.keras.Model` can be trained with the `tf.estimator` API by converting the model to an `tf.estimator.Estimator` object with `tf.keras.estimator.model_to_estimator`. See Creating Estimators from Keras models.\n",
    "\n",
    "Note: Ennable `eager execution` for debugging Estimator input functions and inspecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/dy/_9hwtjyn0vd3d7fj95ry9mgw0000gn/T/tmpjPhOs2\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10d8f9850>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/dy/_9hwtjyn0vd3d7fj95ry9mgw0000gn/T/tmpjPhOs2', '_train_distribute': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model3 = keras.Sequential([keras.layers.Dense(10, activation='softmax'),\n",
    "                           keras.layers.Dense(10, activation='softmax')])\n",
    "\n",
    "model3.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "estimator = keras.estimator.model_to_estimator(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple GPUs\n",
    "`tf.keras` models can run on multiple GPUs using `tf.contrib.distribute.DistributionStrategy`. This API provides distributed training on multiple GPUs with almost no changes to existing code.\n",
    "\n",
    "Currently, `tf.contrib.distribute.MirroredStrategy` is the only supported distribution strategy. MirroredStrategy does in-graph replication with synchronous training using all-reduce on a single machine. To use DistributionStrategy with Keras, convert the `tf.keras.Model` to a `tf.estimator.Estimator` with `tf.keras.estimator.model_to_estimator`, then train the estimator\n",
    "\n",
    "The following example distributes a `tf.keras.Model` across multiple GPUs on a single machine.\n",
    "\n",
    "First, define a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = keras.Sequential()\n",
    "model4.add(keras.layers.Dense(16, activation='relu', input_shape=(10,)))\n",
    "model4.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "model4.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an *input pipeline*. The `input_fn` returns a `tf.data.Dataset` object userd to distribute the data across multiple devices - with each device processing a slice of the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    x = np.random.random((1024, 10))\n",
    "    y = np.random.random(2, size=(1024, 1))\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.repeat(10)\n",
    "    dataset = dataset.batch(32)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a `tf.estimator.RunConfig` and set the *train_distribute* argument to the `tf.contrib.distribute.MirroredStrategy` instance. When creating MirroredStrategy, you can specify a list of devices or set the *num_gpus* argument. The default uses all available GPUs, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Must specify at least one device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-6fe8a2acc54b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'pinfo tf.contrib.distribute.MirroredStrategy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_distribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/leo/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_gpus, cross_tower_ops, prefetch_on_device)\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must only specify one of `devices` and `num_gpus`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Must specify at least one device.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     assert len(set(devices)) == len(devices), (\n\u001b[1;32m     85\u001b[0m         \"No duplicates allowed in `devices` argument.\")\n",
      "\u001b[0;31mAssertionError\u001b[0m: Must specify at least one device."
     ]
    }
   ],
   "source": [
    "?tf.contrib.distribute.MirroredStrategy(devices=localhost)\n",
    "strategy = tf.contrib.distribute.MirroredStrategy()\n",
    "config = tf.estimator.RunConfig(train_distribute=strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Keras model to a `tf.estimator.Estimator` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a58f786cfb04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m keras_estimator = keras.estimator.model_to_estimator(\n\u001b[1;32m      2\u001b[0m     \u001b[0mkeras_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_dir'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "keras_estimator = keras.estimator.model_to_estimator(\n",
    "    keras_model=model4,\n",
    "    config=config,\n",
    "    model_dir='./model_dir'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, train the `Estimator` instance by providing the `input_fn` and `steps` arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras_estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-843363108fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'keras_estimator' is not defined"
     ]
    }
   ],
   "source": [
    "keras_estimator.train(input_fn=input_fn, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
